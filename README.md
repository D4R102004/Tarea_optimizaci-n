# Tarea_optimizaciÃ³n

Este proyecto corresponde a la **Tarea Evaluativa** de la asignatura de Modelos de OptimizaciÃ³n.  
Su objetivo es analizar y resolver un **problema de optimizaciÃ³n no lineal en dos variables**, aplicando dos mÃ©todos clÃ¡sicos de optimizaciÃ³n sin restricciones:  
el **Gradiente Descendente** y el **MÃ©todo de Newton**.

---

## ğŸ“– DescripciÃ³n del problema

Se busca minimizar la siguiente funciÃ³n:

\[
f(x, y) = (2x^3y - y^3)^2 + x^2
\]

El problema es continuo, coercivo y no convexo, pero presenta un Ãºnico mÃ­nimo global en el punto \((x, y) = (0, 0)\).

---

## âš™ï¸ Estructura del proyecto

Tarea_Optimizacion/
â”‚
â”œâ”€â”€ algoritmos/
â”‚ â”œâ”€â”€ gradiente_descendente.py
â”‚ â”œâ”€â”€ newton.py
â”‚ â”œâ”€â”€ guardar_resultados.py
â”‚
â”œâ”€â”€ funcion/
â”‚ â””â”€â”€ function.py

â”œâ”€â”€ graficar/
â”‚ â””â”€â”€ grafico.py
â”‚
â”œâ”€â”€ experimentos.py
â”œâ”€â”€ analizar_resultados.py
â”œâ”€â”€ main.py
â”œâ”€â”€ resultados.json
â”œâ”€â”€ resultados_completos.json
â””â”€â”€ README.md


---

## ğŸ§  Algoritmos implementados

- **Gradiente Descendente:** mÃ©todo de primer orden basado en la direcciÃ³n opuesta al gradiente.
- **MÃ©todo de Newton:** mÃ©todo de segundo orden que usa la matriz Hessiana para ajustar el paso y la direcciÃ³n.

Ambos mÃ©todos se implementaron manualmente utilizando **Autograd** para el cÃ¡lculo automÃ¡tico de derivadas y **NumPy** para las operaciones vectoriales.

---

## ğŸ§ª EjecuciÃ³n del proyecto

1. Crear un entorno virtual y activar:
   ```bash
   python -m venv venv
   source venv/bin/activate     # Linux/Mac
   venv\Scripts\activate        # Windows

Instalar dependencias:
pip install numpy autograd matplotlib

Ejecutar los experimentos y anÃ¡lisis:
python main.py
